{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1bda7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os \n",
    "import csv\n",
    "import wandb\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from timm import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "import timm\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler, default_collate\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "from Dataset_ML import *\n",
    "from utils_ML import *\n",
    "\n",
    "from models_ML3_v1 import *\n",
    "from models_ML3_v2 import *\n",
    "from models_ML3_v3 import *\n",
    "from models_ML3_v4__ import *\n",
    "from models_ML3_v4___ import *\n",
    "from models_ML3_v4____ import *\n",
    "\n",
    "from Transformer_USVN import Transformer_USVN\n",
    "from BiLSTM_USVN import BiLSTM_USVN\n",
    "from cnnlstm import CNNLSTM\n",
    "from cnntransformer import CNNTransformer\n",
    "from C3D_model import C3D\n",
    "from R2Plus1D_model import R2Plus1DClassifier\n",
    "\n",
    "import vidaug.augmentors as va\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "import math\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce3b183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_all_seeds(SEED):\n",
    "    # REPRODUCIBILITY\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(SEED)\n",
    "    random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ef6da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_parameters(module):\n",
    "    return sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "\n",
    "def collate_video(batch_list):\n",
    "    \"\"\"\n",
    "    A custom collate function to be passed to the callate_fn argument when creating a pytorch dataloader.\n",
    "    This is necessary because videos have different lengths. We handle by combining all videos along the time \n",
    "    dimension and returning the number of frames in each video.\n",
    "    \"\"\"\n",
    "    vids = torch.concat([b[0] for b in batch_list])\n",
    "    # num_frames = [b.shape[0] for b in batch_list]\n",
    "    labels = [b[1] for b in batch_list]\n",
    "    paths = [b[2] for b in batch_list]\n",
    "    # record = {\n",
    "    #     'video': vids,\n",
    "    #     'num_frames': num_frames\n",
    "    # }\n",
    "\n",
    "    # use pytorch's default collate function for remaining items\n",
    "    # for b in batch_list:\n",
    "    #     b.pop('video')\n",
    "    # record.update(default_collate(batch_list))\n",
    "\n",
    "    return vids, labels, paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38a9a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class attention_video_dataset(Dataset):\n",
    "    \"\"\" Video Dataset.\n",
    "    \n",
    "    \"\"\"\n",
    "    # def __init__(self, class0_csv_path, class1_csv_path, class2_csv_path, class3_csv_path, transforms, padding_type, is_train=True): # case 1\n",
    "    def __init__(self, csv_path, transforms, img_size, is_train=True): \n",
    "\n",
    "        # class 0 / class 1, class 2 / class 3\n",
    "        self.csv_path = csv_path\n",
    "        \n",
    "        self.video_df = pd.read_csv(self.csv_path)\n",
    "\n",
    "        self.transforms = transforms\n",
    "        self.is_train = is_train\n",
    "\n",
    "        self.video_path_list = [str(i) for i in self.video_df[f'{img_size}_clip_path']] \n",
    "        \n",
    "        # 4 artifacts class\n",
    "        self.PRED_LABEL = [\n",
    "            'A-line_lbl',\n",
    "            'total-B-line_lbl',\n",
    "            'Consolidation_lbl',\n",
    "            'Pleural effusion_lbl'\n",
    "            ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "         \n",
    "        clip_path = self.video_path_list[idx]\n",
    "        sampled_clip = load_video(self.video_path_list[idx])\n",
    "\n",
    "        if self.is_train:\n",
    "            # apply augmentation\n",
    "            sometimes = lambda aug: va.Sometimes(0.5, aug)\n",
    "\n",
    "            sigma = 0.7\n",
    "            seq = va.Sequential([ # randomly rotates the video with a degree randomly choosen from [-10, 10]  \n",
    "                sometimes(va.HorizontalFlip()),\n",
    "                sometimes(va.RandomRotate(degrees=10))\n",
    "            ])\n",
    "            sampled_clip = np.array(seq(sampled_clip))\n",
    "        \n",
    "        augmented_images = []\n",
    "        for frame in sampled_clip:\n",
    "            augmented_image = torch.from_numpy(self.transforms(image=frame)['image']).permute(2, 0, 1)\n",
    "            augmented_images.append(augmented_image)\n",
    "            \n",
    "        torch_auged_clip = torch.concat([f[None] for f in augmented_images])\n",
    "\n",
    "        label = torch.FloatTensor(np.zeros(len(self.PRED_LABEL), dtype=float))\n",
    "        \n",
    "        for i in range(0, len(self.PRED_LABEL)):\n",
    "            if (self.video_df[self.PRED_LABEL[i].strip()].iloc[idx].astype('float') > 0):\n",
    "                label[i] = self.video_df[self.PRED_LABEL[i].strip()].iloc[idx].astype('float')\n",
    "        \n",
    "        return torch_auged_clip, label, clip_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8fb5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_all_seeds(1234)\n",
    "\n",
    "# Set up model\n",
    "# model_version = 'v1'\n",
    "# pooling_method = 'attn_multilabel'\n",
    "\n",
    "# model_version = 'v2'\n",
    "# pooling_method = 'attn_multilabel_conv'\n",
    "\n",
    "# model_version = 'v3'\n",
    "# pooling_method = 'attn_multilabel_conv'\n",
    "\n",
    "# model_version = 'v4'\n",
    "# pooling_method = 'attn_multilabel_conv'\n",
    "\n",
    "# model_version = 'v4_'\n",
    "# pooling_method = 'attn_multilabel_conv'\n",
    "\n",
    "# model_version = 'v4__'\n",
    "# pooling_method = 'attn_multilabel_conv'\n",
    "\n",
    "# model_version = 'v4___'\n",
    "# pooling_method = 'attn_multilabel_conv'\n",
    "\n",
    "model_version = 'v4____'\n",
    "pooling_method = 'attn_multilabel_conv'\n",
    "\n",
    "# model_version = 'v5'\n",
    "# pooling_method = 'attn_multilabel_conv'\n",
    "\n",
    "# model_version = 'v6'\n",
    "# pooling_method = 'attn_multilabel_conv'\n",
    "\n",
    "# pooling_method = 'attn'\n",
    "# pooling_method = 'max'\n",
    "# pooling_method = 'avg'\n",
    "\n",
    "num_heads = 8\n",
    "k_size = 13\n",
    "\n",
    "batch_size=1\n",
    "\n",
    "encoder = timm.create_model('densenet161', pretrained=False, num_classes=0)\n",
    "\n",
    "num_frames = [30]*batch_size\n",
    "if model_version == 'v1':\n",
    "    model = MedVidNet_multi_attn(encoder, num_heads, pooling_method = pooling_method)\n",
    "elif model_version == 'v2':\n",
    "    model = MedVidNet_multi_attn_conv(encoder, num_heads, pooling_method = pooling_method)\n",
    "elif model_version == 'v3':\n",
    "    model = MedVidNet_multi_attn_conv2(encoder, num_heads, pooling_method = pooling_method)\n",
    "elif model_version == 'v4':\n",
    "    model = MedVidNet_multi_attn_conv3(encoder, num_heads, pooling_method = pooling_method, kernel_width= k_size)\n",
    "elif model_version == 'v4_':\n",
    "    model = MedVidNet_multi_attn_conv3_(encoder, num_heads, pooling_method = pooling_method, kernel_width= k_size)\n",
    "elif model_version == 'v4__':\n",
    "    model = MedVidNet_multi_attn_conv3__(encoder, num_heads, pooling_method = pooling_method, kernel_width= k_size)\n",
    "elif model_version == 'v4___':\n",
    "    model = MedVidNet_multi_attn_conv3___(encoder, num_heads, pooling_method = pooling_method, kernel_width= k_size)\n",
    "elif model_version == 'v4____':\n",
    "    model = MedVidNet_multi_attn_conv3____(encoder, num_heads, pooling_method = pooling_method, kernel_width= k_size)\n",
    "elif model_version == 'v5':\n",
    "    model = MedVidNet_multi_attn_conv4(encoder, num_heads, pooling_method = pooling_method, kernel_width= k_size)\n",
    "elif model_version == 'v6':\n",
    "    model = MedVidNet_multi_attn_conv5(encoder, num_heads, pooling_method = pooling_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b19354",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# load weight\n",
    "fold_num = 3\n",
    "\n",
    "chk_std = \"loss\"\n",
    "\n",
    "lr = '1e-06'\n",
    "\n",
    "version = 'version_1'\n",
    "train_layer = \"all\"\n",
    "\n",
    "# model_name = 'LUVM'\n",
    "model_name = 'LUV_Net'\n",
    "\n",
    "# model_name = 'USVN'\n",
    "# model_name = 'C3D'\n",
    "# model_name = 'R2Plus1D'\n",
    "# model_name = 'Transformer_USVN'\n",
    "# model_name = 'CNNLSTM'\n",
    "# model_name = 'CNNTransformer'\n",
    "\n",
    "# encoder_name = 'densenet161'\n",
    "# encoder_name = 'mae_densenet161'\n",
    "encoder_name = 'imgnet_init_densenet161'\n",
    "\n",
    "model_test_rate = \"0.2\"\n",
    "\n",
    "data_type = \"before_all_data\"\n",
    "\n",
    "encoder_batch_size = 32\n",
    "video_batch_size = 4\n",
    "\n",
    "model_output_class = 5\n",
    "img_size = 256\n",
    "\n",
    "gpu_index = 3\n",
    "device = torch.device(f\"cuda:{gpu_index}\" if torch.cuda.is_available() else \"cpu\")\n",
    "# weight_path = f'/data2/hoon2/Results/video_model2/seed234_test{model_test_rate}_std_{chk_std}_{data_type}_{version}_{train_layer}_{model_output_class}_artifacts_duplicate_batch{video_batch_size}_256_30frame_{model_name}_{model_version}_{encoder_name}_{encoder_batch_size}_{pooling_method}_{num_heads}head_{k_size}ksize_fold{fold_num}_lr{lr}_checkpoint'\n",
    "weight_path = f'/data2/hoon2/Results/video_model2/seed234_test{model_test_rate}_std_{chk_std}_{data_type}_{version}_{train_layer}_{model_output_class}_artifacts_duplicate_batch{video_batch_size}_256_30frame_{model_name}_{encoder_name}_{encoder_batch_size}_{pooling_method}_{num_heads}head_{k_size}ksize_fold{fold_num}_lr{lr}_checkpoint'\n",
    "\n",
    "check_point = torch.load(weight_path, map_location=device)\n",
    "\n",
    "# torch.nn.DataParallel을 사용하여 모델을 학습하고 저장한 경우에 이러한 접두어가 자주 발생\n",
    "if 'module' in list(check_point['model'].keys())[0]:\n",
    "    # If so, remove the 'module.' prefix from the keys in the state_dict\n",
    "    new_state_dict = {k[7:]: v for k, v in check_point['model'].items()}\n",
    "    model.load_state_dict(new_state_dict)\n",
    "else:\n",
    "    # If not using DataParallel, simply load the state_dict\n",
    "    model.load_state_dict(check_point['model'])\n",
    "\n",
    "model = model.to(device)\n",
    "best_val_thres = check_point['best_valid_thres']\n",
    "\n",
    "num_pars = num_parameters(model)\n",
    "num_pars_encoder = num_parameters(encoder)\n",
    "print(f\"Number of trainable params: {num_pars} ({num_pars - num_pars_encoder} excluding encoder).\")\n",
    "print(f\"Number of encoder params: {num_pars_encoder}\")\n",
    "print(f\"Number of excluding encoder: {num_pars - num_pars_encoder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e71214",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 256\n",
    "\n",
    "base_path = f'/data2/hoon2/LUS_Dataset/csv_files/temporally_separated_test_set'\n",
    "\n",
    "test_csv_path = os.path.join(base_path, f'clip/{model_output_class}_artifacts/temporally_separated_test.csv')\n",
    "    \n",
    "# dataset\n",
    "test_dataset = attention_video_dataset(test_csv_path, transforms = apply_transforms(mode=None), img_size = img_size, is_train = False)\n",
    "# dataloader\n",
    "test_dataloader =  torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle = False, collate_fn=collate_video, drop_last=False)\n",
    "\n",
    "#len_dataloader\n",
    "len_test_dataset = len(test_dataloader.dataset)\n",
    "print(\"Test dataset size:\", len_test_dataset)\n",
    "print(\"Test data loader size:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613d33a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(test_csv_path, index_col = False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d251ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_thres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2844543f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_dataloader.dataset[10][0].shape)\n",
    "print(test_dataloader.dataset[10][1])\n",
    "print(test_dataloader.dataset[10][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ddd242",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_label(test_label):\n",
    "    # 각 인덱스에 해당하는 레이블의 이름\n",
    "    label_names = ['A-line', 'B-lines', 'Consolidation', 'Pleural effusion']\n",
    "    \n",
    "    # test_label이 tensor인지 확인하고 list로 변환\n",
    "    if isinstance(test_label, torch.Tensor):\n",
    "        test_label = test_label.tolist()\n",
    "    \n",
    "    labels = [label_names[idx] for idx, value in enumerate(test_label) if value == 1.0]\n",
    "    \n",
    "    return f\"{', '.join(labels)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49b628b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_heads = 16\n",
    "# k_size = 5\n",
    "\n",
    "save_dir = f'/home/work/LUS/Results/attention_output/temporally_separated_test/{model_name}_{model_version}_{video_batch_size}batch_{encoder_name}_{num_heads}head_{k_size}ksize_fold{fold_num}'\n",
    "\n",
    "model.eval() \n",
    "with torch.no_grad():\n",
    "    for data in tqdm(test_dataloader, desc=\"Testing\", unit=\"batch\"):    \n",
    "        test_img, test_label, test_path = data\n",
    "        \n",
    "        file_name = test_path[0].split('/')[-1].split('.')[0]\n",
    "        test_img, test_label = test_img.float().to(device), test_label\n",
    "\n",
    "        test_output, attentions = model(test_img, num_frames)\n",
    "\n",
    "        sigmoid = nn.Sigmoid()\n",
    "        test_output_ = sigmoid(test_output)\n",
    "    #     print(test_output)\n",
    "        test_preds_np = test_output_[0].data.cpu().numpy()\n",
    "\n",
    "        test_preds_np = np.where(test_preds_np >= best_val_thres, 1, 0)\n",
    "\n",
    "        # 각 레이블에 대해서 attention 값의 평균을 통해 각각 프레임의 중요도를 계산\n",
    "        top_k = 5\n",
    "        num_labels = 4\n",
    "        label_names = ['A-line', 'B-lines', 'Consolidation', 'Pleural effusion']\n",
    "\n",
    "        fig, axs = plt.subplots(4, 5, figsize=(15, 12))\n",
    "\n",
    "        for label_idx in range(num_labels):\n",
    "            # 현재 레이블의 attention 값을 추출\n",
    "            frame_attention_sum = attentions[label_idx][:, 0].sum(dim=1)  # torch.Size([30])\n",
    "            frame_attention_mean = frame_attention_sum / num_heads\n",
    "\n",
    "            # 상위 top_k 프레임 인덱스 선택\n",
    "            top_frame_indices = torch.topk(frame_attention_mean, k=top_k).indices\n",
    "            top_frame_indices = top_frame_indices[torch.argsort(top_frame_indices)]  # 인덱스를 오름차순으로 정렬\n",
    "\n",
    "            # 상위 프레임을 plot\n",
    "            for i, idx in enumerate(top_frame_indices):\n",
    "                ax = axs[label_idx, i]  \n",
    "                ax.imshow(test_img[idx, 0].cpu(), cmap='gray')  \n",
    "                ax.set_title(f\"Frame {idx.item()} ({frame_attention_mean[idx]:.2f})\", fontsize=11)\n",
    "                ax.axis('off')  \n",
    "\n",
    "            # 각 행의 첫 번째 열 위에 레이블 이름 추가\n",
    "            axs[label_idx, 0].text(\n",
    "                -0.3, 0.5, label_names[label_idx], transform=axs[label_idx, 0].transAxes,\n",
    "                fontsize=14, fontweight='bold', ha='center', va='center', rotation=90, color='black'\n",
    "            )\n",
    "\n",
    "        # Set the title\n",
    "        int_test_label_lst = test_label[0].int().tolist()\n",
    "#         print('label:', int_test_label_lst)\n",
    "#         print('pred:', test_preds_np)\n",
    "        result = get_video_label(test_label[0])\n",
    "\n",
    "        # 전체 그래프 제목 추가\n",
    "        fig.suptitle(f\"{file_name}'s Top-5 Frames for Each Label(video labels : {result})\", fontsize=16)\n",
    "\n",
    "        plt.subplots_adjust(wspace=0.3, hspace=0.5)  \n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save the plot\n",
    "        os.makedirs(save_dir, exist_ok=True)  \n",
    "        save_path = os.path.join(save_dir, f'{file_name}_{int_test_label_lst}_{list(test_preds_np)}.png')\n",
    "        plt.savefig(save_path, bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abae2977",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50919d9b",
   "metadata": {},
   "source": [
    "# attention plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e046d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# single pattern example\n",
    "# plot_file_name = '61398238_00008_121_150' # A-line\n",
    "# plot_file_name = '52426014_00009_25_54' # B-line\n",
    "\n",
    "# multi patterns example\n",
    "plot_file_name = '44937986_00003_73_102' \n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(test_dataloader, desc=\"Testing\", unit=\"batch\"):\n",
    "        test_img, test_label, test_path = data\n",
    "\n",
    "        # Check if the file name matches `plot_file_name`\n",
    "        file_name = test_path[0].split('/')[-1].split('.')[0]\n",
    "        if plot_file_name in file_name:\n",
    "            print(f\"Processing file: {file_name}\")\n",
    "\n",
    "            test_img, test_label = test_img.float().to(device), test_label\n",
    "\n",
    "            # Get model outputs and attentions\n",
    "            test_output, attentions = model(test_img, num_frames)\n",
    "\n",
    "            sigmoid = nn.Sigmoid()\n",
    "            test_output_ = sigmoid(test_output)\n",
    "            test_preds_np = test_output_[0].data.cpu().numpy()\n",
    "            test_preds_np = np.where(test_preds_np >= best_val_thres, 1, 0)\n",
    "\n",
    "            # Parameters\n",
    "            top_k = 3  # Number of top frames\n",
    "            num_labels = 4\n",
    "            label_names = ['A-line', 'B-lines', 'Consolidation', 'Pleural effusion']\n",
    "\n",
    "            # Create figure for attention maps\n",
    "            fig, axs = plt.subplots(2, 2, figsize=(16, 10))  # 2행 2열로 플롯\n",
    "            axs = axs.flatten()  # Flatten for easier indexing\n",
    "\n",
    "            # Initialize dictionary to store top-k frame indices per label\n",
    "            top_k_frames = {}\n",
    "\n",
    "            for label_idx in range(num_labels):\n",
    "                # Extract attention scores\n",
    "                frame_attention_sum = attentions[label_idx][:, 0].sum(dim=1).cpu().numpy()\n",
    "                frame_attention_mean = frame_attention_sum / num_heads\n",
    "\n",
    "                # Find top-k indices\n",
    "                top_indices = np.argsort(frame_attention_mean)[-top_k:]\n",
    "                top_k_frames[label_idx] = top_indices  # Save for later use\n",
    "\n",
    "                # Plot attention scores\n",
    "                ax = axs[label_idx]\n",
    "                ax.plot(frame_attention_mean, label='Attention Score', color='blue', linewidth=2)\n",
    "\n",
    "                # Highlight top-k indices\n",
    "                for idx in top_indices:\n",
    "                    ax.scatter(\n",
    "                        idx, frame_attention_mean[idx], color='red', s=100, edgecolors='white', linewidth=2,\n",
    "                        label='Top-k' if idx == top_indices[0] else \"\"\n",
    "                    )\n",
    "\n",
    "                # Customize the plot\n",
    "                ax.set_title(f\"Attention Scores for {label_names[label_idx]}\", fontsize=14, fontweight='bold')\n",
    "                ax.set_xlabel(\"Frame Index\", fontsize=12)\n",
    "                ax.set_ylabel(\"Attention Score\", fontsize=12)\n",
    "                ax.grid(True)\n",
    "                \n",
    "                # Add legend to the first plot only\n",
    "                ax.legend(loc='lower right', fontsize=10)\n",
    "#                 if label_idx == 0:\n",
    "#                     ax.legend(loc='upper right', fontsize=10)\n",
    "\n",
    "            # Adjust layout and add a main title\n",
    "            plt.tight_layout()\n",
    "            plt.subplots_adjust(top=0.9)\n",
    "            fig.suptitle(f\"Attention Scores for {plot_file_name}\", fontsize=16, fontweight='bold')\n",
    "\n",
    "            # Show attention map plots\n",
    "            plt.show()\n",
    "\n",
    "            # Plot top-k frames for each label\n",
    "            fig, axs = plt.subplots(num_labels, top_k, figsize=(16, 4 * num_labels))  # num_labels행, top_k열\n",
    "\n",
    "            for label_idx, indices in top_k_frames.items():\n",
    "                for i, frame_idx in enumerate(indices):\n",
    "                    ax = axs[label_idx, i] if num_labels > 1 else axs[i]  # Handle single-row case\n",
    "                    frame = test_img[frame_idx, 0].cpu().numpy()  # Extract frame as numpy array\n",
    "\n",
    "                    # Plot the frame\n",
    "                    ax.imshow(frame, cmap='gray')\n",
    "\n",
    "                    # Add title with frame index and attention score\n",
    "                    ax.set_title(f\"Label: {label_names[label_idx]}\\nFrame {frame_idx} (Score: {frame_attention_mean[frame_idx]:.3f})\",\n",
    "                                 fontsize=10)\n",
    "                    ax.axis('off')\n",
    "\n",
    "            # Add main title for the frames plot\n",
    "            plt.tight_layout()\n",
    "            plt.subplots_adjust(top=0.9)\n",
    "#             plt.subplots_adjust(wspace=0.2, hspace=0.3)\n",
    "            fig.suptitle(f\"Top-{top_k} Frames for Each Label ({plot_file_name})\", fontsize=16, fontweight='bold')\n",
    "\n",
    "            # Show top-k frame plots\n",
    "            plt.show()\n",
    "\n",
    "            break  # Exit after processing the matching file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16295160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define multiple ground truth ranges\n",
    "\n",
    "# single pattern example\n",
    "# gt_ranges = [(1, 20)] # A-line\n",
    "# gt_ranges = [(1, 13), (23, 30)] # B-line\n",
    "\n",
    "# multi patterns example\n",
    "# gt_ranges = [(3, 30)] # consolidation\n",
    "gt_ranges = [(7, 30)] # effusion\n",
    "\n",
    "label_idx = 3\n",
    "\n",
    "# Parameters\n",
    "top_k = 3  # Number of top frames\n",
    "label_names = ['A-line', 'B-lines', 'Consolidation', 'Pleural effusion']\n",
    "\n",
    "# Compute frame attention scores\n",
    "frame_attention_sum = attentions[label_idx][:, 0].sum(dim=1).cpu().numpy()\n",
    "frame_attention_scaled = (frame_attention_sum - frame_attention_sum.min()) / (frame_attention_sum.max() - frame_attention_sum.min())\n",
    "\n",
    "# Initialize the gt_line with zeros\n",
    "gt_line = np.zeros(len(frame_attention_scaled))  # Length matches the number of frames\n",
    "\n",
    "# Set the ground truth to 1 for each range\n",
    "for start, end in gt_ranges:\n",
    "    gt_line[start-1:end] = 1\n",
    "    \n",
    "# Find top-k indices based on scaled attention\n",
    "top_indices = np.argsort(frame_attention_scaled)[-top_k:]\n",
    "\n",
    "# Plot: Scaled attention scores\n",
    "# fig, ax = plt.subplots(figsize=(12, 6))\n",
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "# ax.plot(range(1, len(frame_attention_scaled) + 1), frame_attention_scaled, \n",
    "#         label='Scaled Attention Score', color='blue', linewidth=2)\n",
    "ax.plot(range(1, len(frame_attention_scaled) + 1), frame_attention_scaled, \n",
    "        label='Scaled Attention Score', color='orange', linewidth=2)\n",
    "ax.plot(range(1, len(gt_line) + 1), gt_line, \n",
    "        label='GT (Ground Truth)', color='green', linestyle='--', linewidth=2)\n",
    "for idx in top_indices:\n",
    "    ax.scatter(idx + 1, frame_attention_scaled[idx], color='red', s=100, \n",
    "               edgecolors='black', linewidth=2,\n",
    "               label='Top-k' if idx == top_indices[0] else \"\")\n",
    "\n",
    "# ax.set_title(\"Scaled Attention Scores (0-1 range)\", fontsize=14)\n",
    "# ax.set_title(f\"Scaled Attention Scores for {label_names[label_idx]}\", fontsize=25, fontweight='bold')\n",
    "ax.set_title(f\"Scaled Attention Scores for {label_names[label_idx]}\", fontsize=25)\n",
    "ax.set_xlabel(\"Frame Index\", fontsize=25)\n",
    "ax.set_ylabel(\"Scaled Attention Score\", fontsize=25)\n",
    "ax.grid(True)\n",
    "# ax.legend(loc='upper right', fontsize=20)\n",
    "ax.legend(loc='lower right', fontsize=20)\n",
    "\n",
    "# Adjust layout and add a main title\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.85)\n",
    "# plt.suptitle(f\"Scaled Attention Scores for {label_names[label_idx]}\\n{plot_file_name}\", \n",
    "#              fontsize=14, fontweight='bold')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f50bb74",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187406a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d11bed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
