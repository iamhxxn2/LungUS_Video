{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc30439a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os \n",
    "import csv\n",
    "import wandb\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from timm import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "import timm\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler, default_collate\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "from Dataset_ML import *\n",
    "from utils_ML import *\n",
    "\n",
    "from models_ML3_v1 import *\n",
    "from models_ML3_v2 import *\n",
    "from models_ML3_v3 import *\n",
    "from models_ML3_v4 import *\n",
    "from models_ML3_v4__ import *\n",
    "from models_ML3_v4___ import *\n",
    "from models_ML3_v4____ import *\n",
    "from models_ML3_v5 import *\n",
    "from models_ML3_v6 import *\n",
    "\n",
    "from Transformer_USVN import Transformer_USVN\n",
    "from BiLSTM_USVN import BiLSTM_USVN\n",
    "from cnnlstm import CNNLSTM\n",
    "from cnntransformer import CNNTransformer\n",
    "from C3D_model import C3D\n",
    "from R2Plus1D_model import R2Plus1DClassifier\n",
    "\n",
    "import vidaug.augmentors as va\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "import math\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f744640d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_all_seeds(SEED):\n",
    "    # REPRODUCIBILITY\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(SEED)\n",
    "    random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3ceb2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_parameters(module):\n",
    "    return sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "\n",
    "def collate_video(batch_list):\n",
    "    \"\"\"\n",
    "    A custom collate function to be passed to the callate_fn argument when creating a pytorch dataloader.\n",
    "    This is necessary because videos have different lengths. We handle by combining all videos along the time \n",
    "    dimension and returning the number of frames in each video.\n",
    "    \"\"\"\n",
    "    vids = torch.concat([b[0] for b in batch_list])\n",
    "    # num_frames = [b.shape[0] for b in batch_list]\n",
    "    labels = [b[1] for b in batch_list]\n",
    "    paths = [b[2] for b in batch_list]\n",
    "    # record = {\n",
    "    #     'video': vids,\n",
    "    #     'num_frames': num_frames\n",
    "    # }\n",
    "\n",
    "    # use pytorch's default collate function for remaining items\n",
    "    # for b in batch_list:\n",
    "    #     b.pop('video')\n",
    "    # record.update(default_collate(batch_list))\n",
    "\n",
    "    return vids, labels, paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b75bb901",
   "metadata": {},
   "outputs": [],
   "source": [
    "class attention_video_dataset(Dataset):\n",
    "    \"\"\" Video Dataset.\n",
    "    \n",
    "    \"\"\"\n",
    "    # def __init__(self, class0_csv_path, class1_csv_path, class2_csv_path, class3_csv_path, transforms, padding_type, is_train=True): # case 1\n",
    "    def __init__(self, csv_path, transforms, img_size, is_train=True): \n",
    "\n",
    "        # class 0 / class 1, class 2 / class 3\n",
    "        self.csv_path = csv_path\n",
    "        \n",
    "        self.video_df = pd.read_csv(self.csv_path)\n",
    "\n",
    "        self.transforms = transforms\n",
    "        self.is_train = is_train\n",
    "\n",
    "        self.video_path_list = [str(i) for i in self.video_df[f'{img_size}_clip_path']] \n",
    "        \n",
    "        # 4 artifacts class\n",
    "        self.PRED_LABEL = [\n",
    "            'A-line_lbl',\n",
    "            'total-B-line_lbl',\n",
    "            'Consolidation_lbl',\n",
    "            'Pleural effusion_lbl'\n",
    "            ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "         \n",
    "        clip_path = self.video_path_list[idx]\n",
    "        sampled_clip = load_video(self.video_path_list[idx])\n",
    "\n",
    "        if self.is_train:\n",
    "            # apply augmentation\n",
    "            sometimes = lambda aug: va.Sometimes(0.5, aug)\n",
    "\n",
    "            sigma = 0.7\n",
    "            seq = va.Sequential([ # randomly rotates the video with a degree randomly choosen from [-10, 10]  \n",
    "                sometimes(va.HorizontalFlip()),\n",
    "                sometimes(va.RandomRotate(degrees=10))\n",
    "            ])\n",
    "            sampled_clip = np.array(seq(sampled_clip))\n",
    "        \n",
    "        augmented_images = []\n",
    "        for frame in sampled_clip:\n",
    "            augmented_image = torch.from_numpy(self.transforms(image=frame)['image']).permute(2, 0, 1)\n",
    "            augmented_images.append(augmented_image)\n",
    "            \n",
    "        torch_auged_clip = torch.concat([f[None] for f in augmented_images])\n",
    "\n",
    "        label = torch.FloatTensor(np.zeros(len(self.PRED_LABEL), dtype=float))\n",
    "        \n",
    "        for i in range(0, len(self.PRED_LABEL)):\n",
    "            if (self.video_df[self.PRED_LABEL[i].strip()].iloc[idx].astype('float') > 0):\n",
    "                label[i] = self.video_df[self.PRED_LABEL[i].strip()].iloc[idx].astype('float')\n",
    "        \n",
    "        return torch_auged_clip, label, clip_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45cab03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_all_seeds(1234)\n",
    "\n",
    "# Set up model\n",
    "# model_version = 'v1'\n",
    "# pooling_method = 'attn_multilabel'\n",
    "\n",
    "# model_version = 'v2'\n",
    "# pooling_method = 'attn_multilabel_conv'\n",
    "\n",
    "# model_version = 'v3'\n",
    "# pooling_method = 'attn_multilabel_conv'\n",
    "\n",
    "# model_version = 'v4'\n",
    "# pooling_method = 'attn_multilabel_conv'\n",
    "\n",
    "# model_version = 'v4_'\n",
    "# pooling_method = 'attn_multilabel_conv'\n",
    "\n",
    "# model_version = 'v4__'\n",
    "# pooling_method = 'attn_multilabel_conv'\n",
    "\n",
    "# model_version = 'v4___'\n",
    "# pooling_method = 'attn_multilabel_conv'\n",
    "\n",
    "model_version = 'v4____'\n",
    "pooling_method = 'attn_multilabel_conv'\n",
    "\n",
    "# model_version = 'v5'\n",
    "# pooling_method = 'attn_multilabel_conv'\n",
    "\n",
    "# model_version = 'v6'\n",
    "# pooling_method = 'attn_multilabel_conv'\n",
    "\n",
    "# pooling_method = 'attn'\n",
    "# pooling_method = 'max'\n",
    "# pooling_method = 'avg'\n",
    "\n",
    "num_heads = 8\n",
    "k_size = 13\n",
    "\n",
    "batch_size=1\n",
    "\n",
    "encoder = timm.create_model('densenet161', pretrained=False, num_classes=0)\n",
    "\n",
    "num_frames = [30]*batch_size\n",
    "if model_version == 'v1':\n",
    "    model = MedVidNet_multi_attn(encoder, num_heads, pooling_method = pooling_method)\n",
    "elif model_version == 'v2':\n",
    "    model = MedVidNet_multi_attn_conv(encoder, num_heads, pooling_method = pooling_method)\n",
    "elif model_version == 'v3':\n",
    "    model = MedVidNet_multi_attn_conv2(encoder, num_heads, pooling_method = pooling_method)\n",
    "elif model_version == 'v4':\n",
    "    model = MedVidNet_multi_attn_conv3(encoder, num_heads, pooling_method = pooling_method, kernel_width= k_size)\n",
    "elif model_version == 'v4_':\n",
    "    model = MedVidNet_multi_attn_conv3_(encoder, num_heads, pooling_method = pooling_method, kernel_width= k_size)\n",
    "elif model_version == 'v4__':\n",
    "    model = MedVidNet_multi_attn_conv3__(encoder, num_heads, pooling_method = pooling_method, kernel_width= k_size)\n",
    "elif model_version == 'v4___':\n",
    "    model = MedVidNet_multi_attn_conv3___(encoder, num_heads, pooling_method = pooling_method, kernel_width= k_size)\n",
    "elif model_version == 'v4____':\n",
    "    model = MedVidNet_multi_attn_conv3____(encoder, num_heads, pooling_method = pooling_method, kernel_width= k_size)\n",
    "elif model_version == 'v5':\n",
    "    model = MedVidNet_multi_attn_conv4(encoder, num_heads, pooling_method = pooling_method, kernel_width= k_size)\n",
    "elif model_version == 'v6':\n",
    "    model = MedVidNet_multi_attn_conv5(encoder, num_heads, pooling_method = pooling_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e106927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable params: 136222884 (109750884 excluding encoder).\n",
      "Number of encoder params: 26472000\n",
      "Number of excluding encoder: 109750884\n"
     ]
    }
   ],
   "source": [
    "################################\n",
    "# load weight\n",
    "fold_num = 3\n",
    "\n",
    "chk_std = \"loss\"\n",
    "\n",
    "lr = '1e-06'\n",
    "\n",
    "version = 'version_1'\n",
    "train_layer = \"all\"\n",
    "\n",
    "model_name = 'LUVM'\n",
    "# model_name = 'USVN'\n",
    "# model_name = 'C3D'\n",
    "# model_name = 'R2Plus1D'\n",
    "# model_name = 'Transformer_USVN'\n",
    "# model_name = 'CNNLSTM'\n",
    "# model_name = 'CNNTransformer'\n",
    "\n",
    "# encoder_name = 'densenet161'\n",
    "# encoder_name = 'mae_densenet161'\n",
    "encoder_name = 'imgnet_init_densenet161'\n",
    "\n",
    "model_test_rate = \"0.2\"\n",
    "\n",
    "data_type = \"before_all_data\"\n",
    "\n",
    "encoder_batch_size = 32\n",
    "video_batch_size = 4\n",
    "\n",
    "model_output_class = 5\n",
    "img_size = 256\n",
    "\n",
    "gpu_index = 1\n",
    "device = torch.device(f\"cuda:{gpu_index}\" if torch.cuda.is_available() else \"cpu\")\n",
    "weight_path = f'/home/work/LUS/Results/video_model2/seed234_test{model_test_rate}_std_{chk_std}_{data_type}_{version}_{train_layer}_{model_output_class}_artifacts_duplicate_batch{video_batch_size}_256_30frame_{model_name}_{model_version}_{encoder_name}_{encoder_batch_size}_{pooling_method}_{num_heads}head_{k_size}ksize_fold{fold_num}_lr{lr}_checkpoint'\n",
    "# weight_path = f'/home/work/LUS/Results/clip_base/multilabel_classification/test{model_test_rate}_std_{chk_std}_{data_type}_{version}_{train_layer}_{model_output_class}_artifacts_duplicate_batch{video_batch_size}_256_30frame_{model_name}_{encoder_name}_{encoder_batch_size}_{pooling_method}_fold{fold_num}_checkpoint'\n",
    "\n",
    "check_point = torch.load(weight_path, map_location=device)\n",
    "\n",
    "# torch.nn.DataParallel을 사용하여 모델을 학습하고 저장한 경우에 이러한 접두어가 자주 발생\n",
    "if 'module' in list(check_point['model'].keys())[0]:\n",
    "    # If so, remove the 'module.' prefix from the keys in the state_dict\n",
    "    new_state_dict = {k[7:]: v for k, v in check_point['model'].items()}\n",
    "    model.load_state_dict(new_state_dict)\n",
    "else:\n",
    "    # If not using DataParallel, simply load the state_dict\n",
    "    model.load_state_dict(check_point['model'])\n",
    "\n",
    "model = model.to(device)\n",
    "best_val_thres = check_point['best_valid_thres']\n",
    "\n",
    "num_pars = num_parameters(model)\n",
    "num_pars_encoder = num_parameters(encoder)\n",
    "print(f\"Number of trainable params: {num_pars} ({num_pars - num_pars_encoder} excluding encoder).\")\n",
    "print(f\"Number of encoder params: {num_pars_encoder}\")\n",
    "print(f\"Number of excluding encoder: {num_pars - num_pars_encoder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c936979d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset size: 474\n",
      "Test data loader size: 474\n"
     ]
    }
   ],
   "source": [
    "base_path = f'/home/work/LUS/Dataset/csv_files/clip_multilabel_classification'\n",
    "\n",
    "test_csv_path = os.path.join(base_path, f'{data_type}/{version}/{model_output_class}_artifacts/test_{model_test_rate}/fold_{fold_num}/test.csv')\n",
    "    \n",
    "# dataset\n",
    "test_dataset = attention_video_dataset(test_csv_path, transforms = apply_transforms(mode=None), img_size = img_size, is_train = False)\n",
    "\n",
    "# dataloader\n",
    "test_dataloader =  torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle = False, collate_fn=collate_video, drop_last=False)\n",
    "\n",
    "#len_dataloader\n",
    "len_test_dataset = len(test_dataloader.dataset)\n",
    "print(\"Test dataset size:\", len_test_dataset)\n",
    "print(\"Test data loader size:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9a12ff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>256_clip_path</th>\n",
       "      <th>512_clip_path</th>\n",
       "      <th>PatientID</th>\n",
       "      <th>study_id</th>\n",
       "      <th>A-line_lbl</th>\n",
       "      <th>B-line_lbl</th>\n",
       "      <th>Confluent B-line_lbl</th>\n",
       "      <th>Consolidation_lbl</th>\n",
       "      <th>Pleural effusion_lbl</th>\n",
       "      <th>raw_avi_path</th>\n",
       "      <th>Lung ultrasound score_lbl</th>\n",
       "      <th>total-B-line_lbl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/work/LUS/Dataset/clip_avi_dataset/256/ve...</td>\n",
       "      <td>/home/work/LUS/Dataset/clip_avi_dataset/512/ve...</td>\n",
       "      <td>30625107</td>\n",
       "      <td>30625107_3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/work/LUS/Dataset/processed_dataset_avi/3...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/work/LUS/Dataset/clip_avi_dataset/256/ve...</td>\n",
       "      <td>/home/work/LUS/Dataset/clip_avi_dataset/512/ve...</td>\n",
       "      <td>30625107</td>\n",
       "      <td>30625107_3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/work/LUS/Dataset/processed_dataset_avi/3...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/work/LUS/Dataset/clip_avi_dataset/256/ve...</td>\n",
       "      <td>/home/work/LUS/Dataset/clip_avi_dataset/512/ve...</td>\n",
       "      <td>30625107</td>\n",
       "      <td>30625107_3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/work/LUS/Dataset/processed_dataset_avi/3...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/work/LUS/Dataset/clip_avi_dataset/256/ve...</td>\n",
       "      <td>/home/work/LUS/Dataset/clip_avi_dataset/512/ve...</td>\n",
       "      <td>30625107</td>\n",
       "      <td>30625107_5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/work/LUS/Dataset/processed_dataset_avi/3...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/work/LUS/Dataset/clip_avi_dataset/256/ve...</td>\n",
       "      <td>/home/work/LUS/Dataset/clip_avi_dataset/512/ve...</td>\n",
       "      <td>30625107</td>\n",
       "      <td>30625107_5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/work/LUS/Dataset/processed_dataset_avi/3...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>/home/work/LUS/Dataset/clip_avi_dataset/256/ve...</td>\n",
       "      <td>/home/work/LUS/Dataset/clip_avi_dataset/512/ve...</td>\n",
       "      <td>59646042</td>\n",
       "      <td>59646042_10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/work/LUS/Dataset/processed_dataset_avi/5...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>/home/work/LUS/Dataset/clip_avi_dataset/256/ve...</td>\n",
       "      <td>/home/work/LUS/Dataset/clip_avi_dataset/512/ve...</td>\n",
       "      <td>59646042</td>\n",
       "      <td>59646042_10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/work/LUS/Dataset/processed_dataset_avi/5...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>/home/work/LUS/Dataset/clip_avi_dataset/256/ve...</td>\n",
       "      <td>/home/work/LUS/Dataset/clip_avi_dataset/512/ve...</td>\n",
       "      <td>59646042</td>\n",
       "      <td>59646042_10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/work/LUS/Dataset/processed_dataset_avi/5...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>/home/work/LUS/Dataset/clip_avi_dataset/256/ve...</td>\n",
       "      <td>/home/work/LUS/Dataset/clip_avi_dataset/512/ve...</td>\n",
       "      <td>59646042</td>\n",
       "      <td>59646042_10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/work/LUS/Dataset/processed_dataset_avi/5...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>/home/work/LUS/Dataset/clip_avi_dataset/256/ve...</td>\n",
       "      <td>/home/work/LUS/Dataset/clip_avi_dataset/512/ve...</td>\n",
       "      <td>59646042</td>\n",
       "      <td>59646042_10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/work/LUS/Dataset/processed_dataset_avi/5...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>474 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         256_clip_path  \\\n",
       "0    /home/work/LUS/Dataset/clip_avi_dataset/256/ve...   \n",
       "1    /home/work/LUS/Dataset/clip_avi_dataset/256/ve...   \n",
       "2    /home/work/LUS/Dataset/clip_avi_dataset/256/ve...   \n",
       "3    /home/work/LUS/Dataset/clip_avi_dataset/256/ve...   \n",
       "4    /home/work/LUS/Dataset/clip_avi_dataset/256/ve...   \n",
       "..                                                 ...   \n",
       "469  /home/work/LUS/Dataset/clip_avi_dataset/256/ve...   \n",
       "470  /home/work/LUS/Dataset/clip_avi_dataset/256/ve...   \n",
       "471  /home/work/LUS/Dataset/clip_avi_dataset/256/ve...   \n",
       "472  /home/work/LUS/Dataset/clip_avi_dataset/256/ve...   \n",
       "473  /home/work/LUS/Dataset/clip_avi_dataset/256/ve...   \n",
       "\n",
       "                                         512_clip_path  PatientID  \\\n",
       "0    /home/work/LUS/Dataset/clip_avi_dataset/512/ve...   30625107   \n",
       "1    /home/work/LUS/Dataset/clip_avi_dataset/512/ve...   30625107   \n",
       "2    /home/work/LUS/Dataset/clip_avi_dataset/512/ve...   30625107   \n",
       "3    /home/work/LUS/Dataset/clip_avi_dataset/512/ve...   30625107   \n",
       "4    /home/work/LUS/Dataset/clip_avi_dataset/512/ve...   30625107   \n",
       "..                                                 ...        ...   \n",
       "469  /home/work/LUS/Dataset/clip_avi_dataset/512/ve...   59646042   \n",
       "470  /home/work/LUS/Dataset/clip_avi_dataset/512/ve...   59646042   \n",
       "471  /home/work/LUS/Dataset/clip_avi_dataset/512/ve...   59646042   \n",
       "472  /home/work/LUS/Dataset/clip_avi_dataset/512/ve...   59646042   \n",
       "473  /home/work/LUS/Dataset/clip_avi_dataset/512/ve...   59646042   \n",
       "\n",
       "        study_id  A-line_lbl  B-line_lbl  Confluent B-line_lbl  \\\n",
       "0     30625107_3           1           0                     0   \n",
       "1     30625107_3           1           0                     0   \n",
       "2     30625107_3           1           0                     0   \n",
       "3     30625107_5           1           0                     0   \n",
       "4     30625107_5           1           0                     0   \n",
       "..           ...         ...         ...                   ...   \n",
       "469  59646042_10           0           0                     0   \n",
       "470  59646042_10           0           0                     0   \n",
       "471  59646042_10           0           0                     0   \n",
       "472  59646042_10           0           0                     0   \n",
       "473  59646042_10           0           0                     0   \n",
       "\n",
       "     Consolidation_lbl  Pleural effusion_lbl  \\\n",
       "0                    0                     0   \n",
       "1                    0                     0   \n",
       "2                    0                     0   \n",
       "3                    0                     0   \n",
       "4                    0                     0   \n",
       "..                 ...                   ...   \n",
       "469                  0                     1   \n",
       "470                  0                     1   \n",
       "471                  0                     1   \n",
       "472                  0                     1   \n",
       "473                  0                     1   \n",
       "\n",
       "                                          raw_avi_path  \\\n",
       "0    /home/work/LUS/Dataset/processed_dataset_avi/3...   \n",
       "1    /home/work/LUS/Dataset/processed_dataset_avi/3...   \n",
       "2    /home/work/LUS/Dataset/processed_dataset_avi/3...   \n",
       "3    /home/work/LUS/Dataset/processed_dataset_avi/3...   \n",
       "4    /home/work/LUS/Dataset/processed_dataset_avi/3...   \n",
       "..                                                 ...   \n",
       "469  /home/work/LUS/Dataset/processed_dataset_avi/5...   \n",
       "470  /home/work/LUS/Dataset/processed_dataset_avi/5...   \n",
       "471  /home/work/LUS/Dataset/processed_dataset_avi/5...   \n",
       "472  /home/work/LUS/Dataset/processed_dataset_avi/5...   \n",
       "473  /home/work/LUS/Dataset/processed_dataset_avi/5...   \n",
       "\n",
       "     Lung ultrasound score_lbl  total-B-line_lbl  \n",
       "0                            0                 0  \n",
       "1                            0                 0  \n",
       "2                            0                 0  \n",
       "3                            0                 0  \n",
       "4                            0                 0  \n",
       "..                         ...               ...  \n",
       "469                          3                 0  \n",
       "470                          3                 0  \n",
       "471                          3                 0  \n",
       "472                          3                 0  \n",
       "473                          3                 0  \n",
       "\n",
       "[474 rows x 12 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(test_csv_path, index_col = False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e6a05ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2747937, 0.28761905, 0.22301994, 0.054686785]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_val_thres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbc808a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 3, 256, 256])\n",
      "tensor([0., 0., 1., 1.])\n",
      "/home/work/LUS/Dataset/clip_avi_dataset/256/version_1/30625107_00009_25_54.avi\n"
     ]
    }
   ],
   "source": [
    "print(test_dataloader.dataset[10][0].shape)\n",
    "print(test_dataloader.dataset[10][1])\n",
    "print(test_dataloader.dataset[10][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21ee6212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_label(test_label):\n",
    "    # 각 인덱스에 해당하는 레이블의 이름\n",
    "    label_names = ['A-line', 'B-lines', 'Consolidation', 'Pleural effusion']\n",
    "    \n",
    "    # test_label이 tensor인지 확인하고 list로 변환\n",
    "    if isinstance(test_label, torch.Tensor):\n",
    "        test_label = test_label.tolist()\n",
    "    \n",
    "    labels = [label_names[idx] for idx, value in enumerate(test_label) if value == 1.0]\n",
    "    \n",
    "    return f\"{', '.join(labels)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1897ce10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 3, 256, 256])\n",
      "tensor([0., 0., 1., 1.])\n",
      "/home/work/LUS/Dataset/clip_avi_dataset/256/version_1/30625107_00009_25_54.avi\n"
     ]
    }
   ],
   "source": [
    "print(test_dataloader.dataset[10][0].shape)\n",
    "print(test_dataloader.dataset[10][1])\n",
    "print(test_dataloader.dataset[10][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1bb7892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093d91d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_heads = 16\n",
    "# k_size = 5\n",
    "\n",
    "save_dir = f'/home/work/LUS/Results/attention_output/internal_test/{model_name}_{model_version}_{video_batch_size}batch_{encoder_name}_{num_heads}head_{k_size}ksize_fold{fold_num}'\n",
    "\n",
    "model.eval() \n",
    "with torch.no_grad():\n",
    "    for data in tqdm(test_dataloader, desc=\"Testing\", unit=\"batch\"):    \n",
    "        test_img, test_label, test_path = data\n",
    "        \n",
    "        '''\n",
    "        # Assuming the model and get_video_label function are already defined\n",
    "        plot_and_save_topk_frames(\n",
    "            test_img=test_img, \n",
    "            test_label=test_label[0], \n",
    "            test_path=test_path[0], \n",
    "            best_val_thres = best_val_thres,\n",
    "            num_frames=num_frames, \n",
    "            model=model, \n",
    "            get_video_label=get_video_label,\n",
    "            save_dir=save_root_dir,  # Specify the save directory\n",
    "        )\n",
    "        file_name = test_path.split('/')[-1].split('.')[0]\n",
    "        '''\n",
    "        file_name = test_path[0].split('/')[-1].split('.')[0]\n",
    "        test_img, test_label = test_img.float().to(device), test_label\n",
    "\n",
    "        test_output, attentions = model(test_img, num_frames)\n",
    "\n",
    "        sigmoid = nn.Sigmoid()\n",
    "        test_output_ = sigmoid(test_output)\n",
    "    #     print(test_output)\n",
    "        test_preds_np = test_output_[0].data.cpu().numpy()\n",
    "\n",
    "        test_preds_np = np.where(test_preds_np >= best_val_thres, 1, 0)\n",
    "\n",
    "        # 각 레이블에 대해서 attention 값의 평균을 통해 각각 프레임의 중요도를 계산\n",
    "        top_k = 5\n",
    "        num_labels = 4\n",
    "        label_names = ['A-line', 'B-lines', 'Consolidation', 'Pleural effusion']\n",
    "\n",
    "        fig, axs = plt.subplots(4, 5, figsize=(15, 12))\n",
    "\n",
    "        for label_idx in range(num_labels):\n",
    "            # 현재 레이블의 attention 값을 추출\n",
    "            frame_attention_sum = attentions[label_idx][:, 0].sum(dim=1)  # torch.Size([30])\n",
    "            frame_attention_mean = frame_attention_sum / num_heads\n",
    "\n",
    "            # 상위 top_k 프레임 인덱스 선택\n",
    "            top_frame_indices = torch.topk(frame_attention_mean, k=top_k).indices\n",
    "            top_frame_indices = top_frame_indices[torch.argsort(top_frame_indices)]  # 인덱스를 오름차순으로 정렬\n",
    "\n",
    "            # 상위 프레임을 plot\n",
    "            for i, idx in enumerate(top_frame_indices):\n",
    "                ax = axs[label_idx, i]  \n",
    "                ax.imshow(test_img[idx, 0].cpu(), cmap='gray')  \n",
    "                ax.set_title(f\"Frame {idx.item()} ({frame_attention_mean[idx]:.2f})\", fontsize=11)\n",
    "                ax.axis('off')  \n",
    "\n",
    "            # 각 행의 첫 번째 열 위에 레이블 이름 추가\n",
    "            axs[label_idx, 0].text(\n",
    "                -0.3, 0.5, label_names[label_idx], transform=axs[label_idx, 0].transAxes,\n",
    "                fontsize=14, fontweight='bold', ha='center', va='center', rotation=90, color='black'\n",
    "            )\n",
    "\n",
    "        # Set the title\n",
    "        int_test_label_lst = test_label[0].int().tolist()\n",
    "#         print('label:', int_test_label_lst)\n",
    "#         print('pred:', test_preds_np)\n",
    "        result = get_video_label(test_label[0])\n",
    "\n",
    "        # 전체 그래프 제목 추가\n",
    "        fig.suptitle(f\"{file_name}'s Top-5 Frames for Each Label(video labels : {result})\", fontsize=16)\n",
    "\n",
    "        plt.subplots_adjust(wspace=0.3, hspace=0.5)  \n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save the plot\n",
    "        os.makedirs(save_dir, exist_ok=True)  \n",
    "        save_path = os.path.join(save_dir, f'{file_name}_{int_test_label_lst}_{list(test_preds_np)}.png')\n",
    "        plt.savefig(save_path, bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2992c86a",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72da2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() \n",
    "with torch.no_grad():\n",
    "#     test_img, test_label = batch\n",
    "    test_img, test_label, test_path = test_dataloader.dataset[0]\n",
    "    \n",
    "    test_img, test_label = test_img.float().to(device), test_label\n",
    "\n",
    "    test_output, attentions = model(test_img, num_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a80112",
   "metadata": {},
   "outputs": [],
   "source": [
    "attentions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6806ad97",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_output.shape)\n",
    "print(test_output)\n",
    "print(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb35862",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output\n",
    "sigmoid = nn.Sigmoid()\n",
    "test_output2 = sigmoid(test_output)\n",
    "print(test_output2)\n",
    "test_preds_np = test_output2[0].data.cpu().numpy()\n",
    "\n",
    "test_preds_np = np.where(test_preds_np >= best_val_thres, 1, 0)\n",
    "print(test_preds_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facdeaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = ['A-line', 'B-lines', 'Consolidation', 'Pleural effusion']\n",
    "\n",
    "# 그래프 크기 설정 (4행 1열 레이아웃)\n",
    "fig, axes = plt.subplots(4, 1, figsize=(8, 8))  # 너비 8, 높이 8로 설정\n",
    "\n",
    "# Attention 데이터를 numpy 형식으로 변환\n",
    "np_attentions = [att[:, 0].cpu().numpy() for att in attentions]  # 각 레이블별 attention 값 저장\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    for jx in range(num_heads):  # 헤드 수만큼 그래프를 그립니다.\n",
    "        ax.plot(np_attentions[i][:, jx])\n",
    "#         ax.plot(np_attentions[i][:, jx], label=f'Head {jx+1}')\n",
    "    \n",
    "    ax.set_title(f\"Attention for {label_names[i]}\")\n",
    "    ax.grid()\n",
    "    ax.legend(fontsize=8, loc='upper right')  # 범례를 작게 설정\n",
    "    ax.set_xlabel(\"Frame Index\")\n",
    "    ax.set_ylabel(\"Attention Score\")\n",
    "\n",
    "# 전체 레이아웃 조정\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653c30bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = get_video_label(test_label)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156c465f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(attentions[0].shape)\n",
    "print(attentions[0][:, 0].shape)\n",
    "frame_attention_sum = attentions[0][:, 0].sum(dim=1)\n",
    "frame_attention_sum.shape\n",
    "frame_attention_sum.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c55762a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 레이블에 대해서 attention 값의 평균을 통해 각각 프레임의 중요도를 계산\n",
    "top_k = 5\n",
    "num_labels = 4\n",
    "label_names = ['A-line', 'B-lines', 'Consolidation', 'Pleural effusion']\n",
    "\n",
    "fig, axs = plt.subplots(4, 5, figsize=(15, 12))\n",
    "\n",
    "for label_idx in range(num_labels):\n",
    "    # 현재 레이블의 attention 값을 추출\n",
    "    frame_attention_sum = attentions[label_idx][:, 0].sum(dim=1)  # torch.Size([30])\n",
    "    frame_attention_mean = frame_attention_sum / num_heads\n",
    "    \n",
    "    # 상위 top_k 프레임 인덱스 선택\n",
    "    top_frame_indices = torch.topk(frame_attention_mean, k=top_k).indices\n",
    "    top_frame_indices = top_frame_indices[torch.argsort(top_frame_indices)]  # 인덱스를 오름차순으로 정렬\n",
    "    \n",
    "    # 상위 프레임을 plot\n",
    "    for i, idx in enumerate(top_frame_indices):\n",
    "        ax = axs[label_idx, i]  \n",
    "        ax.imshow(test_img[idx, 0].cpu(), cmap='gray')  \n",
    "        ax.set_title(f\"Frame {idx.item()} ({frame_attention_mean[idx]:.2f})\", fontsize=11)\n",
    "        ax.axis('off')  \n",
    "\n",
    "    # 각 행의 첫 번째 열 위에 레이블 이름 추가\n",
    "    axs[label_idx, 0].text(\n",
    "        -0.3, 0.5, label_names[label_idx], transform=axs[label_idx, 0].transAxes,\n",
    "        fontsize=14, fontweight='bold', ha='center', va='center', rotation=90, color='black'\n",
    "    )\n",
    "\n",
    "# 전체 그래프 제목 추가\n",
    "fig.suptitle(f\"Top-5 Frames for Each Label(video labels : {result})\", fontsize=16)\n",
    "\n",
    "plt.subplots_adjust(wspace=0.3, hspace=0.5)  \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.2 (NGC 23.11/Python 3.10) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
